PanDA is a Workload Management System (WMS)~\cite{marco2009glite} designed to
support the execution of workloads in grid-like distributed computing
environment via pilots~\cite{turilli2017comprehensive}. A pilot-capable WMS
enables high throughput of task execution via multi-level scheduling while
supporting interoperability across multiple sites. This is particularly
relevant for Large Hadron Collider (LHC) experiments, where millions of tasks
are executed across more than 170 sites of the Worldwide LHC Computing Grid
(WLCG) every month, analyzing and producing petabytes of data. The design of
PanDA WMS started in 2005 to support ATLAS \cite{?}.

% ---------------------------------------------------------------------------
\subsection{Design}
\label{subsec:design}

PanDA's application model renders workloads into tasks which are decomposed
into jobs. Workloads are sets of data and transformations for the data. Tasks
are sets of homogeneous operations performed on data stored in sets of input
files. Tasks are decomposed into jobs, where each job consists of the task's
operations performed on a partition of the task's data. Jobs are distributed
across the available compute resources for concurrent execution.

PanDA's data model has been exclusively motivated by the needs of ATLAS, where
each datum refers to a recorded or simulated measurement of a physical process.
Data are stored in files that are grouped into datasets, with a many-to-many
relationship between files and datasets. Data have both attributes and states.
Raw, reconstruction, and simulation data are all assumed to be distributed
across multiple storage facilities and managed by the ATLAS Distributed Data
Management (DDM)~\cite{garonne2012atlas}. Files required by jobs are assumed to
be replicated over the network as needed, both for input and output data. PanDA
supports provenance and traceability for jobs and data. Attributes enable
provenance by linking jobs and data items, providing information like ownership
or project affiliation. States enable traceability by providing information
about the past and present stages of the execution for each job and data file.

%As with jobs, data have both attributes and states, and some of the attributes
%are shared among events and jobs.

PanDA's execution model is based on four main abstractions: tasks, jobs,
queues, and pilots. Both tasks and jobs are assumed to have attributes and
states and to be queued into a global queue for execution. Prioritization and
binding of jobs are assumed to depend on the attributes of each task and job.
PanDA has one global queue where all jobs are registered and one resource queue
for each target compute resource. PanDA assigns specific sets of jobs from
the global queue to the resource queues, depending on the jobs' requirements
and each resource's capability and availability. When pilots become available
on the target compute resource, PanDA sends jobs on each pilot from the
resource queue associated with that target compute resource.

PanDA's security model is based on separation among authentication,
authorization, and accounting for both single users and groups of users. Both
authentication and authorization are based on digital certificates and on the
virtual organization abstraction~\cite{foster2001anatomy}. 


% ---------------------------------------------------------------------------
\subsection{Implementation and Execution}
\label{subsec:implementation}

The implementation of PanDA WMS consists of several interconnected
subsystems, most of them built from off-the-shelf and Open Source components.
Subsystems communicate via messaging using HyperText Transfer Protocol (HTTP)
and dedicated Application Programming Interfaces (APIs), and each
subsystem is implemented by one or more modules. Databases are used to store
tasks, jobs, input/output data, and information about WLCG sites and compute
resources.

Currently, PanDA's architecture has five main subsystems:
AutoPyFactory~\cite{caballero2012autopyfactory},
JEDI/PanDA Server~\cite{maeno2011overview,borodin2015scaling},
PanDA Monitoring~\cite{klimentov2011atlas},
PanDA Pilot~\cite{nilsson2011atlas}, and Schedconfig~\cite{nilsson2008panda}.
AutoPyFactory is the system used to submit pilots to grid sites. JEDI and PanDA
Server process tasks into jobs and broker their distributed execution. PanDA
Monitoring is a web application to monitor the execution of workloads in a
distributed computing environment. PanDA Pilot is a pilot system that executes
jobs on computing infrastructures, managing the stage-in and stage-out of the
jobs' data. Schedconfig is an information system implemented within PanDA to
store PanDA queue (resource) descriptions. Schedconfig synchronizes with the
ATLAS Grid Information system (AGIS)~\cite{anisenkov2014agis} to obtain
information about distributed resources. Other subsystems are used by some
ATLAS workflows (e.g., ATLAS Event Service~\cite{calafiura2015atlas} and ATLAS
Production System~\cite{borodin2016atlas}), but their discussion is omitted
here because they are irrelevant to understanding how PanDA has been ported to
supercomputers. For a full list of subsystems, see Ref.~\cite{panda-wiki_url}.

Figure~\ref{fig:architecture} shows a diagrammatic representation of PanDA's
main subsystems, highlighting the task execution process while omitting
monitoring details to improve readability. During the first data collection
period at the LHC (LHC Run 1), PanDA required users to perform a static
conversion between tasks and jobs; tasks were described as a set of jobs and
then submitted to the PanDA Server. This introduced inefficiency both with
usability and resource utilization. Ideally, users should conceive analyses in
terms of one or more potentially related tasks, while the workload manager
(i.e., PanDA) should partition tasks into jobs, depending on the amount of data
(i.e., number of input files and events) that need to be processed and the task
requirements.

\begin{figure*}
  \includegraphics[width=0.75\textwidth]{images/PanDA_WMS.pdf}
  \caption{PanDA WMS architecture. Numbers indicate the JEDI-based execution
           process described in section~\ref{subsec:implementation}. Several
           subsystems, components, and architectural and communication details
           are abstracted to improve clarity.}
  \label{fig:architecture}
\end{figure*}

PanDA registers three types of workloads for execution: a set of tasks
submitted by the ATLAS Production system (Fig.~\ref{fig:architecture}:1); a
single user task submitted with a whole dataset
(Fig.~\ref{fig:architecture}:2); and a single job submitted by the user,
usually with a single input file (Fig.~\ref{fig:architecture}:3).

The Job Generator component renders a set of jobs from each registered task
based on the amount of input data and the amount of processed data per job by
taking into account the number of input files and number of events per job from
task parameters. Because jobs are not generated from a task all at once, the
job buffer is kept to a manageable size. This strategy enables the tuning of
task parameters based on initial job results and also task prioritization, if
needed (Fig.~\ref{fig:architecture}:4).

The Job Buffer component stores jobs that are waiting to be bound to a
specific resource for execution (Fig.~\ref{fig:architecture}:5). The Brokerage
component pulls jobs from the Job Buffer (Fig.~\ref{fig:architecture}:6),
binding each job to a compute resource based on job requirements, resource
capability (Fig.~\ref{fig:architecture}:7 and (Fig.~\ref{fig:architecture}:8),
and data availability (Fig.~\ref{fig:architecture}:9). Note that the Resource
Configuration component is also called Schedconfig.

The Job Scheduler component stores bound jobs that are waiting to be scheduled
on the assigned resource and executed (Fig.~\ref{fig:architecture}:10). Before
scheduling each job, the Job Scheduler checks whether input data are available
and, if needed, requests a transfer to the storage associated with the target
resource (Fig.~\ref{fig:architecture}:11).

Meanwhile, AutoPyFactory defines PanDA Pilots based on the number of jobs that
are bound and ready to execute, and it submits these pilots to a Condor-G agent
(Fig.~\ref{fig:architecture}:12). Condor-G schedules these pilots on the
required sites (Fig.~\ref{fig:architecture}:13). Once active, pilots interact
with the Job Dispatcher to pull jobs for execution
(Fig.~\ref{fig:architecture}:14). Depending on task and job parameters, failed
jobs may be rescheduled with a new job identifier
(Fig.~\ref{fig:architecture}:15).

Once all of a task's jobs have been executed and, depending on the failure
policy, all or most of the output data have been collected, a task is marked as
done. At that point, no more jobs will be generated for that task, and ATLAS
Production System or single users will be informed about the completion of the
task (Fig.~\ref{fig:architecture}:16). Tasks can also be marked as failed,
depending on whether a user-defined threshold for number of failures has been
exceeded.


% ---------------------------------------------------------------------------
\subsection{Job State Definitions in PanDA}
\label{subsec:jobstatedefs}

The life cycle of the job in the PanDA system is split into a series of
sequentially changing states. Each state is literally coupled with the PanDA
job status used by the different algorithms and monitoring. The status reflects
the current step of the job processing since the time that the job was
submitted to the system, transferred to the particular resource and finally
executed. Figure~\ref{fig:jobstates} illustrates the life cycle of jobs
submitted to PaNDA WMS.

% For two-column wide figures use
\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=0.75\textwidth]{images/job-state-diagram.png}
% figure caption is below the figure
\caption{This is a job state transitions model diagram for PanDA.}
\label{fig:jobstates}
\end{figure*}

Jobs are injected into the system as a ``job parameters'' object with a
``Pending'' status by JEDI in ATLAS or by the PanDA client otherwise.
Initially, this object is represented by a string containing unsorted
parameters. Next, the string is processed, the parameters of the job are sorted
into dedicated database fields, and the status is changed to ``Defined.'' After
that, the job is processed through the brokerage algorithm and assigned to a
particular resource using a PanDA queue, and the status is changed to
``Assigned''. Then, the status is changed to ``Waiting'' while the PanDA server
checks the availability of the input data and the required software at the
resource before changing the job's status to ``Activated''. An activated job is
a job which is ready to be dispatched to the next corresponding pilot. When the
job is dispatched and taken by the pilot, the job's status is changed to
``Sent''. At this time, the handling of the job processing has not been
delegated to the pilot yet, and thus, the next few job states correspond to the
steps of the job processing on the assigned resource. When the job status
changes to ``Starting'', the pilot is starting the job on a worker node or
local batch system, after which the status becomes ``Running'' when the job
begins running on a worker node. At this time, the progression of states is
once again handled by the server instead of by the pilot. After the job
finishes executing and output and log files are transferred, the PanDA server
is responsible for registering the files in the file catalog. At the same time,
the pilot returns the final status of the job to the server by communicating
that the job either succeeded or failed. During this process, the job has a
``Holding'' status. The PanDA server checks the output files regularly by using
cron, and it assigns the final ``Finished'' or ``Failed'' status to the job.
There are also some other statuses, the two most important of which are
``Cancelled'' for manually killed jobs and ``Closed'' for jobs which were
terminated by the system before completion so they could be reassigned to other
sites.

% ---------------------------------------------------------------------------
\subsection{Brokerage Characterization}
\label{subsec:brokerage}

Resources (queues) presented in the database together with the wide set of
static parameters such as walltime, CPU cores, memory, disk space etc. Same
parameters can be provided within job definition to specify strict demands to
the resource where the job can be executed. Both resources (queues) and jobs
with parameters stored in the PanDA database.

Also PanDA server maintains in the DB the dynamic information for queues
about the number of defined, activated and running jobs and also the pilots
statistics - number of requests of different types like ``get job'' or
``update job status''.

PanDA Broker - key  component of the BigPanDA workflow automation - is an
intelligent module designed to prioritize and assign PanDA jobs (job passed
the brokerage transitioning from ``defined'' to the ``assigned'' state) to
available compute resources on the basis of job type, software
availability, input data and its locality, real-time job statistics,
available CPU and storage resources and etc. Users are able to specify
explicitly the resource while job submission or they can rely on automated
brokerage engine. Full power of the PanDA brokerage integrated with another
distributed computing and data management tools (internal and external with
respect to the PanDA) is actively used in ATLAS experiment. In this paper we
will present and will benchmark the basic brokerage functionality.

The basic brokerage algorithm works the following way. It takes the lists of
submitted jobs and available queues. Then each job is checked against each
queue by set of parameters if the queue meets the jobs static demands like
number of CPU core or the walltime. All queues passed the round are
proceeding to the short list where for each queue Broker calculates the
weight on the basis of current job statistics for given queue according to
the formula (1). Job finally assigning to the queue with bigger weight.
Weight calculation algorithm fo ATLAS is more complicated and taking into
account clouds default weights, network bandwidth, sharing policies etc.

The basic brokerage algorithm works the following way. Having the list of the
submitted jobs, each job is checked against available resources as shown in
SELECT{\textunderscore}CAND (Alg. ). Available resources presented as the set
of defined PanDA queues: res = queue$_1$, \ldots, queue$_n$. For each queue
in the set (3) we checking if it's satisfying the parameters of job (4).
Successfully passed queues are concatenating to the list of candidate-queues
(5).

SATISFY{\textunderscore}JOB function (Alg. ) is used to check if the queue
attributes can scope job parameters. Set of the job parameters defined as
par$_1$, \ldots, par$_m$ represents the software/hardware demands to the
resource like CPU core count, walltime, SW releases etc. Each of these
parameters can be mapped to the set of queue attributes defined as atr$_1$,
\ldots, atr$_n$, where $n \geq m$. So for each job parameter (2) we check if
it can be satisfied with the corresponding queue attribute (3). Finally queue
passes the test if it copes all the jobs parameters (5).

The procedure SATISFY{\textunderscore}REQ (Alg. ) is responsible to testing
if the value of the job parameter is in the set of allowed values val$_1$,
\ldots, val$_k$ of the queue attribute (2).

%-  NOTE: I think there are indenting mistakes present in the original
%   writing, because things just honestly don't look right to me, but right
%   now, I'm just typesetting what I see in Google Docs.

% I have already included the package required by this. Just put the code
% list.py file (or whatever other name you want to use.)

\lstinputlisting[language=Python, 
         label={lst:list.py}, 
         caption={Caption}]{list.py}

\begin{tabbing}
\hspace{0.5in}\=
     Require: par; atr = (val$_1$, \ldots, val$_k$) \\
  \> Ensure: True or False \\
  \> 1: \hspace{1em}\= procedure SATISFY{\textunderscore}REQ(par, atr) \\
  \> 2:             \> \hspace{1em}\= if par.value in atr then: \\
  \> 3:             \>             \> return True \\
  \> 4:             \>             \> \hspace{0.1in} return False
\end{tabbing}

\begin{tabbing}
\hspace{0.5in}\=
     Require: job = \{par$_1$, \ldots, par$_m$\}; queue = \{atr$_1$, \ldots, atr$_n$\} \\
  \> Ensure: True or False \\
  \> 1: \hspace{1em}\= procedure SATISFY{\textunderscore}JOB(queue, job) \\
  \> 2:             \> \hspace{1em}\= for all par in job do: \\
  \> 3:             \>             \> if SATISFY{\textunderscore}REQ(par, atr)%
= False then \\
  \> 4:             \>             \> \hspace{2em} return False \\
  \> 5:             \>             \> \hspace{1em} return True
\end{tabbing}

\begin{tabbing}
\hspace{0.5in}\=
     Require: job; res = (queue$_1$, \ldots, queue$_n$) \\
  \> Ensure: cand \\
  \> 1: \hspace{1em}\= procedure SELECT{\textunderscore}CAND(job, res) \\
  \> 2:             \> \hspace{1em}\= cand $\leftarrow$ NONE \\
  \> 3:             \> \hspace{1em}\= for all queue in res do: \\
  \> 4:             \>             \> if SATISFY{\textunderscore}JOB(queue, %
job) = True then \\
  \> 5:             \>             \> \hspace{1em} cand $\cup$ queue \\
  \> 6:             \>             \> return True
\end{tabbing}

As it was shown SELECT{\textunderscore}CAND procedure provides generates the
short list of the candidates queues. SELECT{\textunderscore}QUEUE (Alg. )
taking the short list of the candidate-queues as the set queue$_1$, \ldots,
queue$_n$. For each queue (4) Broker calculates the weight (5) on the basis of
current job statistics for given queue according to the formula (1). Job
finally assigning to the queue with bigger weight (6-7). Weight calculation
algorithm fo ATLAS is more complicated and taking into account clouds default
weights, network bandwidth, sharing policies etc

\begin{tabbing}
\hspace{0.5in}\=
     Require: cand = (queue$_1$, \ldots, queue$_n$) \\
  \> Ensure: res{\textunderscore}queue \\
  \> 1: \hspace{1em}\= procedure SELECT{\textunderscore}QUEUE(cand) \\
  \> 2:             \> \hspace{1em}\= res{\textunderscore}queue $\leftarrow$%
queue$_1$ \\
  \> 3:             \>             \> \hspace{0.1in} max{\textunderscore}%
weight $\leftarrow 0$ \\
  \> 4:             \> \hspace{1em}\= for all queue in cand do: \\
  \> 5:             \>             \> queue.weight $\leftarrow$ %
WEIGHT{\textunderscore}CALC(queue) \\
  \> 6:             \>             \> \hspace{1em} if queue.weight $>$ %
max{\textunderscore}weight then \\
  \> 7:             \>             \> \hspace{2em} res{\textunderscore}queue%
$\leftarrow$ queue \\
  \> 8:             \>             \> \hspace{0.5em} return %
res{\textunderscore}queue
\end{tabbing}

\begin{equation}
  \begin{aligned}
    & manyAssigned = \max(1, \min(2, \frac{assigned}{activated})), \\
    & weight = \frac{running + 1}{(activated + assigned + sharing + defined +
        10) * manyAssigned}
  \end{aligned}
\end{equation}

Response time of the brokerage in general can be estimated as (2). Basically it's time the job
transits from ``defined'' to assigned state.  

\begin{equation}
    T = \sum_{i = 1}^{Q} \sum_{j = 1}^{J} T_{ij}
\end{equation}

In formula (2) $Q$ is the number of available queues, $J$ is the number of
concurrently  submitted jobs and $T_{ij}$ is the time to process job $j$ for
queue $i$. The processing time includes the check if queue meet demands of the
job. Then for successfully selected queues the weight is calculating and job
assigning for the queue with bigger weight. Hence the time $T$ can be presented as sum (3).

\begin{equation}
T = t_1 + t_2 + t3_3 + C
\end{equation}

In formula 3, $t_1$ is the time to make checks if queue meet demands of the
job, $t_2$ is the time for weight calculation and finally $t_3$ is the time
spent to assign job to the resulted queue. 

Under the assumption that all jobs can run on the same average number of queues
$N$ then we can transform equation as (4).

\begin{equation}
T = J * \left ( \sum_{i = 1}^{Q - N} t1_j + \sum_{j = 1}^{N} (tmax + t2_j) + t3 \right) + C, t1 < tmax
\end{equation}

Here N is the average number of queues which met all demands of each job.  As shown in the SATISFY{\textunderscore}JOB algorithm the function returns FALSE as soon as the first discrepancy in the job parameter and queue attributes is met. Hence for for all other Q-N queues the time to make checks  t1 will be less than tmax.

Here N is the average number of queues which met all demands of each job. As shown in the SATISFY{\textunderscore}JOB algorithm the function returns False as
soon as the first discrepancy in the job parameter and queue attributes is met.
Hence for for all other Q-N queues the time to make checks t1 will be less than tmax.

Again taking assumption that the times for different queues are equal we can
streamline the equation like (5)

\begin{equation}
  \begin{aligned}
    T =& J * \left(( Q - N) * t1 + N * (tmax + t2) + t3 \right) + C \\
     =& J * \left(Q * t1 + N * (tmax - t1 + t2) + t3 \right) + C,
        \text{where}~(tmax - t1) > 0
  \end{aligned}
\end{equation}

In order to estimate dependency of brokerage response time from the number of
concurrently submitted jobs we deployed a dedicated test instance of PanDA
server at ORNL. PanDA was configured to use ten testing queues. Two of the
queus was configured to provide 8 CPU cores and eight remaining queues provide
2 cores. All other parameters are configured equal for all queues.

Job submission client was configured to generate and send to the server the
lists of equal jobs where each job demands 4 CPU cores. PanDA testing-instance
was adjusted to simulate the brokerage two queues will be selected as meeting
the criteria of cores number. Then due to simulation of job statistics on that
selected queues the jobs will be assigned to the queue with bigger weight.
Brokerage time dependency on number of concurrently submitted jobs is shown in
figure.

%-  The publisher prefers TIFF, and Ruslan and others have provided the images
%   in TIFF as requested. I am not aware of a LaTeX engine that understands
%   TIFF natively, and pasting shell-escape sequences from the internet is a
%   very, very bad idea, so I just converted it to PNG for now and included
%   both in the repository.

% For two-column wide figures use
\begin{figure*}
  \includegraphics[width=0.75\textwidth]{images/Fig2.png}
\caption{Response time dependency on number of concurrently submitted jobs}
\label{fig:brokeragescaling}
\end{figure*}

For this experiment we measured the response time for a jobs to transit from the
``Defined'' status to the ``Activated''. As in the test environment the JEDI
system wasn't used and injection of the jobs was done using the simple python
client interaction with PanDA REST API the first stated of the job indicated in
PanDA is ``Defined'' and corresponds to the creation time. Also during this
measurements we used no-input jobs. Hence the status of the jobs progressed to
the ``Activated'' immediately after ``Defined''. In general the time to check
input files can be considered as constant for the constant number of input
files. So omitting the ``Assigned'' state in this testing environment is
acceptable. 



%-  vim:set syntax=tex:
